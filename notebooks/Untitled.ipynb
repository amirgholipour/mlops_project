{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a9cc6-85cd-436e-b41f-4ea96228a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "from tensorflow import keras\n",
    "\n",
    "random_seed = 44\n",
    "batch_size = 128\n",
    "datasets_bucket = \"tensorflow-text-classification\"\n",
    "preprocessed_data_folder = \"preprocessed-data\"\n",
    "tf_record_file_size = 500\n",
    "# How to access MinIO\n",
    "minio_address = \"http://minio-ml-workshop:9000\"\n",
    "minio_access_key = \"minioadmin\"\n",
    "minio_secret_key = \"minio123\"\n",
    "\n",
    "minioClient = Minio(minio_address,\n",
    "                    access_key=minio_access_key,\n",
    "                    secret_key=minio_secret_key,\n",
    "                    secure=False)\n",
    "\n",
    "try:\n",
    "    minioClient.fget_object(\n",
    "        datasets_bucket,\n",
    "        \"aclImdb_v1.tar.gz\",\n",
    "        \"/tmp/dataset.tar.gz\")\n",
    "except ResponseError as err:\n",
    "    print(err)\n",
    "\n",
    "extract_folder = f\"/tmp/{datasets_bucket}/\"\n",
    "\n",
    "with tarfile.open(\"/tmp/dataset.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_folder)\n",
    "\n",
    "# Pre-Processing\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "dirs_to_read = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "\n",
    "for dir_name in dirs_to_read:\n",
    "    parts = dir_name.split(\"/\")\n",
    "    dataset = parts[1]\n",
    "    label = parts[2]\n",
    "    for filename in os.listdir(os.path.join(extract_folder, dir_name)):\n",
    "        with open(os.path.join(extract_folder, dir_name, filename), \"r\") as f:\n",
    "            content = f.read()\n",
    "            if dataset == \"train\":\n",
    "                train.append({\n",
    "                    \"text\": content,\n",
    "                    \"label\": label\n",
    "                })\n",
    "            elif dataset == \"test\":\n",
    "                test.append({\n",
    "                    \"text\": content,\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "random.Random(random_seed).shuffle(train)\n",
    "random.Random(random_seed).shuffle(test)\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "\n",
    "def _embedded_sentence_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _label_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def encode_label(label):\n",
    "    if label == \"pos\":\n",
    "        return tf.constant([1, 0])\n",
    "    elif label == \"neg\":\n",
    "        return tf.constant([0, 1])\n",
    "\n",
    "\n",
    "def serialize_example(label, sentence_tensor):\n",
    "    feature = {\n",
    "        \"sentence\": _embedded_sentence_feature(sentence_tensor[0]),\n",
    "        \"label\": _label_feature(label),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto\n",
    "\n",
    "\n",
    "def process_examples(records, prefix=\"\"):\n",
    "    starttime = timeit.default_timer()\n",
    "    total_training = len(records)\n",
    "    print(f\"Total of {total_training} elements\")\n",
    "    total_batches = math.floor(total_training / tf_record_file_size)\n",
    "    if total_training % tf_record_file_size != 0:\n",
    "        total_batches += 1\n",
    "    print(f\"Total of {total_batches} files of {tf_record_file_size} records\")\n",
    "\n",
    "    counter = 0\n",
    "    file_counter = 0\n",
    "    buffer = []\n",
    "    file_list = []\n",
    "    for i in range(len(records)):\n",
    "        counter += 1\n",
    "\n",
    "        sentence_embedding = embed([records[i][\"text\"]])\n",
    "        label_encoded = encode_label(records[i][\"label\"])\n",
    "        record = serialize_example(label_encoded, sentence_embedding)\n",
    "        buffer.append(record)\n",
    "\n",
    "        if counter >= tf_record_file_size:\n",
    "            print(f\"Records in buffer {len(buffer)}\")\n",
    "            # save this buffer of examples as a file to MinIO\n",
    "            counter = 0\n",
    "            file_counter += 1\n",
    "            file_name = f\"{prefix}_file{file_counter}.tfrecord\"\n",
    "            with open(file_name, \"w+\") as f:\n",
    "                with tf.io.TFRecordWriter(f.name, options=\"GZIP\") as writer:\n",
    "                    for example in buffer:\n",
    "                        writer.write(example.SerializeToString())\n",
    "            print(f\"file size {os.stat(file_name).st_size}\")\n",
    "            try:\n",
    "                minioClient.fput_object(datasets_bucket, f\"{preprocessed_data_folder}/{file_name}\", file_name)\n",
    "            except ResponseError as err:\n",
    "                print(err)\n",
    "            file_list.append(file_name)\n",
    "            os.remove(file_name)\n",
    "            buffer = []\n",
    "            print(f\"Done with batch {file_counter}/{total_batches} - {timeit.default_timer() - starttime}\")\n",
    "    if len(buffer) > 0:\n",
    "        file_counter += 1\n",
    "        file_name = f\"file{file_counter}.tfrecord\"\n",
    "        with open(file_name, \"w+\") as f:\n",
    "            with tf.io.TFRecordWriter(f.name) as writer:\n",
    "                for example in buffer:\n",
    "                    writer.write(example.SerializeToString())\n",
    "        try:\n",
    "            minioClient.fput_object(datasets_bucket, f\"{preprocessed_data_folder}/{file_name}\", file_name)\n",
    "        except ResponseError as err:\n",
    "            print(err)\n",
    "        file_list.append(file_name)\n",
    "        os.remove(file_name)\n",
    "        buffer = []\n",
    "    print(\"total time is :\", timeit.default_timer() - starttime)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "# process_examples(train, prefix=\"train\")\n",
    "# process_examples(test, prefix=\"test\")\n",
    "print(\"Done!\")\n",
    "\n",
    "# List all training tfrecord files\n",
    "objects = minioClient.list_objects_v2(datasets_bucket, prefix=\"train\")\n",
    "training_files_list = []\n",
    "for obj in objects:\n",
    "    training_files_list.append(obj.object_name)\n",
    "\n",
    "# List all testing tfrecord files\n",
    "objects = minioClient.list_objects_v2(datasets_bucket, prefix=\"test\")\n",
    "testing_files_list = []\n",
    "for obj in objects:\n",
    "    testing_files_list.append(obj.object_name)\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = minio_access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = minio_secret_key\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"S3_ENDPOINT\"] = minio_address\n",
    "os.environ[\"S3_USE_HTTPS\"] = \"0\"\n",
    "os.environ[\"S3_VERIFY_SSL\"] = \"0\"\n",
    "\n",
    "all_training_filenames = [f\"s3://datasets/{f}\" for f in training_files_list]\n",
    "testing_filenames = [f\"s3://datasets/{f}\" for f in testing_files_list]\n",
    "\n",
    "total_train_data_files = math.floor(len(all_training_filenames) * 0.9)\n",
    "if total_train_data_files == len(all_training_filenames):\n",
    "    total_train_data_files -= 1\n",
    "training_files = all_training_filenames[0:total_train_data_files]\n",
    "validation_files = all_training_filenames[total_train_data_files:]\n",
    "\n",
    "# Now let's create the `tf.data` datasets:\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "ignore_order = tf.data.Options()\n",
    "ignore_order.experimental_deterministic = False\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(training_files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "dataset = dataset.with_options(ignore_order)\n",
    "\n",
    "validation = tf.data.TFRecordDataset(validation_files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "validation = validation.with_options(ignore_order)\n",
    "\n",
    "testing_dataset = tf.data.TFRecordDataset(testing_filenames, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "testing_dataset = testing_dataset.with_options(ignore_order)\n",
    "\n",
    "\n",
    "def decode_fn(record_bytes):\n",
    "    schema = {\n",
    "        \"label\": tf.io.FixedLenFeature([2], dtype=tf.int64),\n",
    "        \"sentence\": tf.io.FixedLenFeature([512], dtype=tf.float32),\n",
    "    }\n",
    "\n",
    "    tf_example = tf.io.parse_single_example(record_bytes, schema)\n",
    "    new_shape = tf.reshape(tf_example[\"sentence\"], [1, 512])\n",
    "    label = tf.reshape(tf_example[\"label\"], [1, 2])\n",
    "    return new_shape, label\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=256,\n",
    "        input_shape=(1, 512),\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=16,\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ![Structure of our Deep Learning model](pic2.png)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Let's prepare our datasets for the training stage by having them repeat\n",
    "# themselves a little and batch `128` items at a time\n",
    "\n",
    "\n",
    "mapped_ds = dataset.map(decode_fn)\n",
    "mapped_ds = mapped_ds.repeat(5)\n",
    "mapped_ds = mapped_ds.batch(128)\n",
    "\n",
    "mapped_validation = validation.map(decode_fn)\n",
    "mapped_validation = mapped_validation.repeat(5)\n",
    "mapped_validation = mapped_validation.batch(128)\n",
    "\n",
    "testing_mapped_ds = testing_dataset.map(decode_fn)\n",
    "testing_mapped_ds = testing_mapped_ds.repeat(5)\n",
    "testing_mapped_ds = testing_mapped_ds.batch(128)\n",
    "\n",
    "checkpoint_path = f\"s3://{datasets_bucket}/checkpoints/cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model_note = \"256\"\n",
    "logdir = f\"s3://{datasets_bucket}/logs/imdb/{model_note}-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Finally we will train the model:\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    mapped_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[cp_callback, tensorboard_callback],\n",
    "    validation_data=mapped_validation,\n",
    ")\n",
    "\n",
    "# Now that we have our model, we want to save it to MinIO\n",
    "model.save(f\"s3://{datasets_bucket}/imdb_sentiment_analysis\")\n",
    "\n",
    "testing = model.evaluate(testing_mapped_ds)\n",
    "\n",
    "samples = [\n",
    "    \"This movie sucks\",\n",
    "    \"This was extremely good, I loved it.\",\n",
    "    \"great acting\",\n",
    "    \"terrible acting\",\n",
    "    \"pure kahoot\",\n",
    "    \"I don't know what's the point of this movie, this movie sucks but the acting is great\",\n",
    "    \"This is not a good movie\",\n",
    "]\n",
    "sample_embedded = embed(samples)\n",
    "\n",
    "res = model.predict(sample_embedded)\n",
    "for s in range(len(samples)):\n",
    "    if res[s][0] > res[s][1]:\n",
    "        print(f\"{samples[s]} - positive\")\n",
    "    else:\n",
    "        print(f\"{samples[s]} - negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
