{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from io import StringIO\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.corpus.stopwords.words('english')   \n",
    "stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e5f8c-fafc-48df-861c-4cb9e4b818fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob nltk gensim wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c3f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac49c-80bf-48ad-b041-a7eaf7dcc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35839aee-02f3-41b0-9997-1af90b582a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NLPTextClassification\"\n",
    "EXPERIMENT_NAME = \"DeepLearning\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2201c73-0605-41b1-8f4b-924aae88fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ae1c9-5dcb-4bca-bbe2-7d7e026b4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb98f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"consumer_complaints.csv\")\n",
    "csv_file = client.get_object(\"raw-data-saeed\", \"consumer_complaints.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# df = df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb218b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cd7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adaf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44137f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c265831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()/df.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['complaint_id','date_received','product','issue','company','state','submitted_via','company_response_to_consumer','timely_response','consumer_disputed?','consumer_complaint_narrative']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[pd.notnull(df1['consumer_complaint_narrative'])]\n",
    "# df1 =  df1[:10000]\n",
    "df1.sample(n = 4000)\n",
    "df1.reset_index(drop=True,inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b7027",
   "metadata": {},
   "source": [
    "## EDA\n",
    "We’ll check the disribution of complaints by product category to understand which product received maximum complaints and \n",
    "\n",
    "other products which rarely receive complaints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18,6))\n",
    "sns.countplot(x='product',data=df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c716",
   "metadata": {},
   "source": [
    "From this plot we can see Debt Collection and Mortgage received maximum number of complaints\n",
    "\n",
    "We’ll now analyze the contingency table in form of plot to understand which product has more customer disputes on their complaints after resolving the issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f96d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df1['product'],df1['consumer_disputed?']).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131590",
   "metadata": {},
   "source": [
    "Not much of difference in proportion of disputes raised by complaint for each product category.\n",
    "\n",
    "Checking various plots to identify patterns within data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b25bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date_received'] = pd.to_datetime(df1['date_received'])\n",
    "df1.date_received.min(),df1.date_received.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8928d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['month'] = df1['date_received'].dt.month\n",
    "sns.countplot(x='month',data=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba145e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='timely_response',data=df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d971c",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921d6b5",
   "metadata": {},
   "source": [
    "## Converting Text data to Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45970b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: ' '.join([i.lower() for i in x.split()]))\n",
    "df1['consumer_complaint_narrative'].sample(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d287b",
   "metadata": {},
   "source": [
    "## Removing Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a372420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].str.replace(r'[^\\w\\s]',\"\")\n",
    "df1['consumer_complaint_narrative'].sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25df29",
   "metadata": {},
   "source": [
    "## Text standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Below, we used three normalizazion dictionaries from these links :\n",
    "# #http://www.hlt.utdallas.edu/~yangl/data/Text_Norm_Data_Release_Fei_Liu/\n",
    "# #http://people.eng.unimelb.edu.au/tbaldwin/etc/emnlp2012-lexnorm.tgz\n",
    "# #http://luululu.com/tweet/typo-corpus-r1.txt\n",
    "dico = {}\n",
    "dico1 = open('doc1.txt', 'rb')\n",
    "for word in dico1:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[1]] = word[3]\n",
    "dico1.close()\n",
    "dico2 = open('doc2.txt', 'rb')\n",
    "for word in dico2:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[0]] = word[1]\n",
    "dico2.close()\n",
    "dico3 = open('doc3.txt', 'rb')\n",
    "for word in dico3:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[0]] = word[1]\n",
    "dico3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_std(words):\n",
    "    list_words = words.split()\n",
    "    for i in range(len(list_words)):\n",
    "        if list_words[i] in dico.keys():\n",
    "            list_words[i] = dico[list_words[i]]\n",
    "    return ' '.join(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].apply(txt_std)\n",
    "df1.consumer_complaint_narrative.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace(r\"xx+\\s\",\"\")\n",
    "df1['consumer_complaint_narrative'].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e07ed",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop]))\n",
    "df1['consumer_complaint_narrative'].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9e607",
   "metadata": {},
   "source": [
    "## Correcting Spelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ensure text is standardized before applying this step\n",
    "from textblob import TextBlob\n",
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "df1.consumer_complaint_narrative.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cab14-10d1-4650-ae57-6d5d56a3c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.iloc[:10]['consumer_complaint_narrative'] =df1.iloc[:10]['consumer_complaint_narrative'].apply(lambda x: str(TextBlob(x).correct()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "## Lemmatizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "\n",
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x:' '.join([Word(i).lemmatize() for i in x.split()]))\n",
    "df1.consumer_complaint_narrative.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e937c",
   "metadata": {},
   "source": [
    "# Word Cloud for all Product categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_name in df1['product'].unique():\n",
    "    print(product_name)\n",
    "    all_words = ' '.join([text for text in df1.loc[df1['product'].str.contains(product_name),'consumer_complaint_narrative']])\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad62b0d",
   "metadata": {},
   "source": [
    "### Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2077aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feature engineering of consumer complaint with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea78e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_y = enc.fit_transform(train_y)\n",
    "valid_y = enc.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##tf-idf verctor representation\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df1['consumer_complaint_narrative'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_complaints = np.append(train_x.values,valid_x.values)\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(train_x.values)#total_complaints\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(valid_x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_index = tokenizer.word_index# dictionary containing words and their index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "MAX_SEQUENCE_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d594e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(valid_y)\n",
    "\n",
    "print(enc.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e1151",
   "metadata": {},
   "source": [
    "\n",
    "## CNN w/ Pre-trained word embeddings(GloVe)\n",
    "We’ll use pre-trained embeddings such as Glove which provides word based vector representation trained on a large corpus.\n",
    "\n",
    "It is trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c16ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "f = client.get_object(\"raw-data-saeed\", \"glove.6B.100d.txt\")\n",
    "embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86779d7e",
   "metadata": {},
   "source": [
    "Now lets create the embedding matrix using the word indexer created from tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45826314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42aea74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets check the word embedded vector representation for token ‘loan’ in our embedding matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[(k,v) for k,v in word_index.items() if v==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eea10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[4]  ## word embedded vector representation for token 'loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f142d",
   "metadata": {},
   "source": [
    "Now we load this embedding matrix into an Embedding layer using Sequential API to form a Convolutional NeuralNet model.\n",
    "Dropout is applied between the hidden layers to factor regularization and prevent overfitting of neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(1024, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(512, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(11, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    " # optimizer = tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    " # optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "              optimizer='rmsprop',\n",
    " metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a1450-da79-4f95-a084-1aa9f7efd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437f12a-59c8-4d52-a315-276384a32c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable = True\n",
    "# for layer in model.layers[:-12]:\n",
    "#     layer.trainable =  False\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbeda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2700363",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b20a43",
   "metadata": {},
   "source": [
    "After 3 epochs the CNN tends to be overfitting the training data and therefore we need to implement early stopping to prevent such instances of overfitting and tune the number of epochs during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d59b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee56637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987be029-8061-4094-8a85-32afec8b68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090a549",
   "metadata": {},
   "source": [
    "Now, we’ll initialize our Embedding layer from scratch and learning its weights during training instead of using a pre-trained word embeddings and build a small 1D convnet to solve our classification problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82107e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Embedding layer requires the specification of the vocabulary size (vocab_size), \n",
    "#the size of the real-valued vector space EMBEDDING_DIM = 100,\n",
    "#and the maximum length of input documents max_length .\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "EMBEDDING_DIM = 100\n",
    "max_length = 394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    " 300,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(11, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    " optimizer=\"adamW\",\n",
    " metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=256,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c741366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(Bidirectional(LSTM(100, dropout = 0.2)))\n",
    "model.add(Dense(11,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=128,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "print(\"############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b785252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'light_cc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21c74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d159338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cafbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e8669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e89f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e5075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81616d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf82415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
